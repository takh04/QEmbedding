{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCNN demonstration with Noisy Devices"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Noisemodel with IBMQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noise Model (Fake Provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/opt/homebrew/bin/python3.11' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.11 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "n_qubits = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from qiskit import IBMQ\n",
    "from qiskit.providers.aer.backends import AerSimulator\n",
    "from qiskit.providers.fake_provider import FakeJakarta, FakeMontreal\n",
    "from qiskit_aer.noise import NoiseModel\n",
    "\n",
    "IBMQ.load_account()\n",
    "noisy = FakeJakarta()\n",
    "\n",
    "noise_model = NoiseModel.from_backend(noisy)\n",
    "coupling_map = noisy.configuration().coupling_map\n",
    "basis_gates = noise_model.basis_gates\n",
    "\n",
    "dev_fake = qml.device(\n",
    "    'qiskit.aer',\n",
    "    wires=n_qubits,\n",
    "    shots=1024,\n",
    "    noise_model=noise_model,\n",
    "    coupling_map=coupling_map,\n",
    "    basis_gates=basis_gates\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_tak = \"798c4f141e8fa071c14823a956f816c8dda622f9f00b95cae62cc5b1a3105b4fe4845a234d661f20281f6b77ac0299e3c4367f6a3b58db16c36bcde3a9cb1151\"\n",
    "TOKEN_dkp = \"3ae7ac10f40eb88c7ebb0eca20aa0788e7a96da729e2c3848d9864684362aaf50dfdda7cb3ec1ffd75eb7ed5b44f7c14f2f17419cf600ce14437ee7cd00ac75b\"\n",
    "\n",
    "IBMQ.save_account(token=TOKEN_dkp, overwrite=True)\n",
    "IBMQ.load_account()\n",
    "provider = IBMQ.get_provider(hub='ibm-q-skku')\n",
    "backend = \"ibmq_jakarta\"\n",
    "\n",
    "dev_jakarta = qml.device(\n",
    "    'qiskit.ibmq',\n",
    "    wires=4,\n",
    "    shots=1024,\n",
    "    backend=backend,\n",
    "    provider=provider\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Mapping Circuits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Feature Mapping Circuits. \n",
    "\n",
    "Note: This is 4 qubit feature mapping circuits used for demonstration.\n",
    "\n",
    "Unlike noiseless simulation, last to first qubit CNOT gates are omitted for connectivity.\n",
    "\n",
    "Also number of layers is set to 1 (N_layers = 3 for noiseless simulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import embedding\n",
    "\n",
    "def Noisy_Four_QuantumEmbedding1(input):\n",
    "    for j in range(4):\n",
    "        qml.Hadamard(wires=j)\n",
    "        embedding.exp_Z(input[j], wires=j)\n",
    "    for k in range(3):\n",
    "        embedding.exp_ZZ2(input[k], input[k+1], wires=[k,k+1])\n",
    "    #exp_ZZ2(input[3], input[0], wires=[3, 0])                  Removed for connectivity\n",
    "\n",
    "def Noisy_Four_QuantumEmbedding1_inverse(input):\n",
    "    #exp_ZZ2(input[3], input[0], wires=[3, 0], inverse=True)    Removce for connectivity\n",
    "    for k in reversed(range(3)):\n",
    "        embedding.exp_ZZ2(input[k], input[k+1], wires=[k,k+1], inverse=True)\n",
    "        qml.Barrier()\n",
    "    for j in range(4):\n",
    "        embedding.exp_Z(input[j], wires=j, inverse=True)\n",
    "        qml.Hadamard(wires=j)\n",
    "\n",
    "# Quantum Embedding 2 for model 2\n",
    "def Noisy_Four_QuantumEmbedding2(input):\n",
    "    for j in range(4):\n",
    "        qml.Hadamard(wires=j)\n",
    "        embedding.exp_Z(input[j], wires=j)\n",
    "    for k in range(3):\n",
    "        embedding.exp_ZZ1(input[4+k], wires=[k, k+1])\n",
    "    #exp_ZZ1(input[15], wires=[7,0])                        Removed for connectivity\n",
    "\n",
    "def Noisy_Four_QuantumEmbedding2_inverse(input):\n",
    "    #embedding.exp_ZZ1(input[15], wires=[7,0], inverse=True) Removed for connectivity\n",
    "    for k in reversed(range(3)):\n",
    "        embedding.exp_ZZ1(input[k+4], wires=[k,k+1], inverse=True)\n",
    "    qml.Barrier()\n",
    "    for j in range(4):\n",
    "        embedding.exp_Z(input[j], wires=j, inverse=True)\n",
    "        qml.Hadamard(wires=j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Hybrid Model 1\n",
    "@qml.qnode(dev_fake, interface=\"torch\")\n",
    "def circuit1(inputs): \n",
    "    Noisy_Four_QuantumEmbedding1(inputs[0:4])\n",
    "    Noisy_Four_QuantumEmbedding1_inverse(inputs[4:8])\n",
    "    return qml.probs(wires=range(4))\n",
    "\n",
    "class Noisy_Model1_Fidelity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qlayer1 = qml.qnn.TorchLayer(circuit1, weight_shapes={})\n",
    "        self.linear_relu_stack1 = nn.Sequential(\n",
    "            nn.Linear(4, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 4)\n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.linear_relu_stack1(x1)\n",
    "        x2 = self.linear_relu_stack1(x2)\n",
    "        x = torch.concat([x1, x2], 1)\n",
    "        x = self.qlayer1(x)\n",
    "        return x[:,0]\n",
    "\n",
    "# Hybrid Model 2\n",
    "@qml.qnode(dev_fake, interface=\"torch\")\n",
    "def circuit2(inputs): \n",
    "    Noisy_Four_QuantumEmbedding2(inputs[0:7])\n",
    "    Noisy_Four_QuantumEmbedding2_inverse(inputs[7:14])\n",
    "    return qml.probs(wires=range(4))\n",
    "\n",
    "class Noisy_Model2_Fidelity(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qlayer2 = qml.qnn.TorchLayer(circuit2, weight_shapes={})\n",
    "        self.linear_relu_stack2 = nn.Sequential(\n",
    "            nn.Linear(4, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 7)\n",
    "        )\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.linear_relu_stack2(x1)\n",
    "        x2 = self.linear_relu_stack2(x2)\n",
    "        x = torch.concat([x1, x2], 1)\n",
    "        x = self.qlayer2(x)\n",
    "        return x[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Circuit1: \\n\")\n",
    "print(\n",
    "    qml.draw(\n",
    "        circuit1,\n",
    "        expansion_strategy=\"device\",\n",
    "        show_matrices=False\n",
    "    )([1,2,3,4,1,2,3,4])\n",
    ")\n",
    "\n",
    "print(\"Circuit2: \\n\")\n",
    "print(\n",
    "    qml.draw(\n",
    "        circuit2,\n",
    "        expansion_strategy=\"device\",\n",
    "        show_matrices=False\n",
    "    )([1,2,3,4,5,6,7,1,2,3,4,5,6,7])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/tak/Github/QEmbedding/')\n",
    "import data\n",
    "import training\n",
    "\n",
    "\n",
    "feature_reduction = 'PCA4'\n",
    "classes = [0,1]\n",
    "X_train, X_test, Y_train, Y_test = data.data_load_and_process('mnist', feature_reduction=feature_reduction, classes=classes)\n",
    "\n",
    "N_valid, N_test = 500, 1000\n",
    "X1_new_valid, X2_new_valid, Y_new_valid = training.new_data(N_valid, X_test, Y_test)\n",
    "X1_new_test, X2_new_test, Y_new_test = training.new_data(N_test, X_test, Y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(model_name):\n",
    "    train_loss = []\n",
    "    if model_name == 'Model1_Fidelity':\n",
    "        model = Model1_Fidelity()\n",
    "        PATH = '/Users/tak/Github/QEmbedding/Results/earlystop 10 experiments/Analysis/Model1_Fidelity.pt'\n",
    "    model.train()\n",
    "\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    for it in range(200):\n",
    "        X1_batch, X2_batch, Y_batch = new_data(10, X_train, Y_train)\n",
    "        X1_batch, X2_batch, Y_batch = X1_batch, X2_batch, Y_batch\n",
    "\n",
    "        pred = model(X1_batch, X2_batch)\n",
    "        pred, Y_batch = pred.to(torch.float32), Y_batch.to(torch.float32)\n",
    "        loss = loss_fn(pred, Y_batch)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print(f\"Iterations: {it} Loss: {loss.item()}\")\n",
    "    \n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "#train_models('Model1_Fidelity')\n",
    "train_models('Model1_Fidelity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/Users/tak/Github/QEmbedding/Results/earlystop 10 experiments/Analysis/Model1_Fidelity.pt'\n",
    "X1_test, X0_test = [], []\n",
    "for i in range(len(X_test)):\n",
    "    if Y_test[i] == 1:\n",
    "        X1_test.append(X_test[i])\n",
    "    else:\n",
    "        X0_test.append(X_test[i])\n",
    "X1_test, X0_test = torch.tensor(X1_test), torch.tensor(X0_test)\n",
    "\n",
    "X1_train, X0_train = [], []\n",
    "for i in range(len(X_train)):\n",
    "    if Y_train[i] == 1:\n",
    "        X1_train.append(X_train[i])\n",
    "    else:\n",
    "        X0_train.append(X_train[i])\n",
    "X1_train, X0_train = torch.tensor(X1_train), torch.tensor(X0_train)\n",
    "\n",
    "\n",
    "dev = qml.device('default.qubit', wires=4)\n",
    "@qml.qnode(dev, interface=\"torch\")\n",
    "def Four_Distance(inputs): \n",
    "    Four_QuantumEmbedding(inputs[0:4])\n",
    "    return qml.density_matrix(wires=range(4))\n",
    "\n",
    "class Distances(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.qlayer1_distance = qml.qnn.TorchLayer(Four_Distance, weight_shapes={})\n",
    "        self.linear_relu_stack1 = nn.Sequential(\n",
    "            nn.Linear(4, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 4)\n",
    "        )\n",
    "    def forward(self, x1, x0, Distance, Trained):\n",
    "        if Trained:\n",
    "            x1 = self.linear_relu_stack1(x1)\n",
    "            x0 = self.linear_relu_stack1(x0)\n",
    "        rhos1 = self.qlayer1_distance(x1)\n",
    "        rhos0 = self.qlayer1_distance(x0)\n",
    "        rho1 = torch.sum(rhos1, dim=0) / len(x1)\n",
    "        rho0 = torch.sum(rhos0, dim=0) / len(x0)\n",
    "        rho_diff = rho1 - rho0\n",
    "        if Distance == 'Trace':\n",
    "            eigvals = torch.linalg.eigvals(rho_diff)\n",
    "            return 0.5 * torch.real(torch.sum(torch.abs(eigvals)))\n",
    "\n",
    "Model1_Fidelity_Distance = Distances()\n",
    "Model1_Fidelity_Distance.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distances Before Training\n",
    "Trace_before_traindata = Model1_Fidelity_Distance(X1_train, X0_train, 'Trace', False)\n",
    "Trace_before_testdata = Model1_Fidelity_Distance(X1_test, X0_test, 'Trace', False)\n",
    "print(f\"Trace Distance (Training Data) Before: {Trace_before_traindata}\")\n",
    "print(f\"Trace Distance (Test Data) Before: {Trace_before_testdata}\")\n",
    "\n",
    "# Distances After training with Model1_Fidelity\n",
    "Trace_Fidelity_traindata = Model1_Fidelity_Distance(X1_train, X0_train, 'Trace', True)\n",
    "Trace_Fidelity_testdata = Model1_Fidelity_Distance(X1_test, X0_test, 'Trace', True)\n",
    "print(f\"Trace Distance (Training Data) After Model1 Fidelity: {Trace_Fidelity_traindata}\")\n",
    "print(f\"Trace Distance (Test Data) After Model1 Fidelity: {Trace_Fidelity_testdata}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training QCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = [-1 if y == 0 else 1 for y in Y_train]\n",
    "Y_test = [-1 if y == 0 else 1 for y in Y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class x_transform(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack1 = nn.Sequential(\n",
    "            nn.Linear(4, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(6, 4)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.linear_relu_stack1(x)\n",
    "        return x.detach().numpy()\n",
    "\n",
    "model = x_transform()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "#model.load_state_dict(torch.load(PATH_Model1_HSinner, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statepreparation(x, Trained):\n",
    "    if Trained:\n",
    "        x = model(torch.tensor(x))\n",
    "    Four_QuantumEmbedding(x)\n",
    "\n",
    "def SU_4(params, wires): # 15 params\n",
    "    qml.U3(params[0], params[1], params[2], wires=wires[0])\n",
    "    qml.U3(params[3], params[4], params[5], wires=wires[1])\n",
    "    qml.CNOT(wires=[wires[0], wires[1]])\n",
    "    qml.RY(params[6], wires=wires[0])\n",
    "    qml.RZ(params[7], wires=wires[1])\n",
    "    qml.CNOT(wires=[wires[1], wires[0]])\n",
    "    qml.RY(params[8], wires=wires[0])\n",
    "    qml.CNOT(wires=[wires[0], wires[1]])\n",
    "    qml.U3(params[9], params[10], params[11], wires=wires[0])\n",
    "    qml.U3(params[12], params[13], params[14], wires=wires[1])\n",
    "\n",
    "def U_TTN(params, wires):  # 2 params\n",
    "    qml.RY(params[0], wires=wires[0])\n",
    "    qml.RY(params[1], wires=wires[1])\n",
    "    qml.CNOT(wires=[wires[0], wires[1]])\n",
    "\n",
    "def QCNN_four(params):\n",
    "    param1 = params[0:2]\n",
    "    param2 = params[2:4]\n",
    "    U_TTN(param1, wires=[0, 1])\n",
    "    U_TTN(param1, wires=[2, 3])\n",
    "    U_TTN(param1, wires=[1, 2])\n",
    "    U_TTN(param1, wires=[3, 0])\n",
    "    U_TTN(param2, wires=[0, 2])\n",
    "\n",
    "\n",
    "@qml.qnode(dev_fake)\n",
    "def QCNN_classifier(params, x, Trained):\n",
    "    statepreparation(x, Trained)\n",
    "    qml.Barrier()\n",
    "    QCNN_four(params)\n",
    "    return qml.expval(qml.PauliZ(2))\n",
    "\n",
    "def Linear_Loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l,p in zip(labels, predictions):\n",
    "        loss += 0.5 * (1 - l * p)\n",
    "    return loss / len(labels)\n",
    "\n",
    "def cost(weights, X_batch, Y_batch, Trained):\n",
    "    preds = [QCNN_classifier(weights, x, Trained) for x in X_batch]\n",
    "    return Linear_Loss(Y_batch, preds)\n",
    "\n",
    "steps = 400\n",
    "learning_rate = 0.01\n",
    "batch_size = 10\n",
    "def circuit_training(X_train, Y_train, Trained):\n",
    "\n",
    "    weights = np.random.random(4, requires_grad = True)\n",
    "    opt = qml.NesterovMomentumOptimizer(stepsize=learning_rate)\n",
    "    loss_history = []\n",
    "    for it in range(steps):\n",
    "        batch_index = np.random.randint(0, len(X_train), (batch_size,))\n",
    "        X_batch = [X_train[i] for i in batch_index]\n",
    "        Y_batch = [Y_train[i] for i in batch_index]\n",
    "        weights, cost_new = opt.step_and_cost(lambda v: cost(v, X_batch, Y_batch, Trained),\n",
    "                                                     weights)\n",
    "        loss_history.append(cost_new)\n",
    "        if it % 10 == 0:\n",
    "            print(\"iteration: \", it, \" cost: \", cost_new)\n",
    "    return loss_history, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    qml.draw(\n",
    "        QCNN_classifier,\n",
    "        expansion_strategy=\"device\",\n",
    "        show_matrices=False\n",
    "    )(x=torch.ones(4), params=np.ones(30), Trained=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_not_trained, weights_not_trained = circuit_training(X_train, Y_train, False)\n",
    "loss_trained, weights_trained = circuit_training(X_train, Y_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LB_before_traindata = 0.5 * (1 - 0.3409276604652405)\n",
    "LB_Fidelity_traindata = 0.5 * (1 - 0.9015634655952454)\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "fig, ax = plt.subplots()\n",
    "clrs = sns.color_palette(\"husl\", 4)\n",
    "with sns.axes_style(\"darkgrid\"):\n",
    "    ax.plot(range(len(loss_not_trained)), loss_not_trained, label=\"Without Pre-training\", c=clrs[0])\n",
    "\n",
    "    ax.plot(range(len(loss_trained)), loss_trained, label=\"With Pre-training\", c=clrs[1])\n",
    "\n",
    "    ax.plot(range(400), np.ones(400) * LB_before_traindata, label=\"Lower Bound without Pre-training\", c=clrs[2])\n",
    "    ax.plot(range(400), np.ones(400) * LB_Fidelity_traindata, label=\"Lower Bound with Pre-training\", c=clrs[3])\n",
    "\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"QCNN Loss History Trained with Model1 Fidelity\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(predictions, labels):\n",
    "    acc = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if np.abs(l - p) < 1:\n",
    "            acc = acc + 1\n",
    "    return acc / len(labels)\n",
    "\n",
    "\n",
    "predictions_trained = [QCNN_classifier(weights_trained, x, Trained=True) for x in X_test]\n",
    "predictions_not_trained = [QCNN_classifier(weights_not_trained, x, Trained=False) for x in X_test]\n",
    "\n",
    "accuracy_trained = accuracy_test(predictions_trained, Y_test)\n",
    "accuracy_not_trained = accuracy_test(predictions_not_trained, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" accuracty after pre-training: {accuracy_trained}\")\n",
    "print(f\" accuracty without pre-training: {accuracy_not_trained}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 25 2022, 13:57:33) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
